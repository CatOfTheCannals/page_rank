\section{Desarrollo}

% Deben explicarse los métodos numéricos que utilizaron y su aplicación al problema
% concreto involucrado en el trabajo práctico. Se deben mencionar los pasos que si-
% guieron para implementar los algoritmos, las dificultades que fueron encontrando y la
% descripción de cómo las fueron resolviendo. Explicar también cómo fueron planteadas
% y realizadas las mediciones experimentales. Los ensayos fallidos, hipótesis y conjeturas
% equivocadas, experimentos y métodos malogrados deben figurar en esta sección, con
% una breve explicación de los motivos de estas fallas (en caso de ser conocidas).

	\subsection{Elección de la estructura de datos}

		Analizando las operaciones necesarias para la resolución del problema, pensamos que para el armado del sistema lineal iba a convenir un tipo de estructura y para su efectiva resolución, otra. \\

		Concretamente, notamos que la matriz de conectividad $W$ era multiplicada (por derecha) por una matriz diagonal y luego se le sumaba una matriz identidad $I$: \\

		\begin{equation}
			(I - pWD) x = [W (-pD) + I] x = e
		\end{equation}

		Multiplicar a $W$ por una matriz diagonal $-pD$, es equivalente a multiplicar a cada columna $\vec{w}_i$ de $W$ por el escalar que está en la diagonal de $-pD$: \\

		\begin{equation}
			W (-pD) = \left( \vec{w}_1 \vec{w}_2 \hdots \vec{w}_n \right) (-pD) = \left( -p d_{11} \vec{w}_1 \quad -p d_{22} \vec{w}_2 \quad \hdots \quad -p d_{nn} \vec{w}_n \right)
		\end{equation}

		Viendo esto, pensamos que sería bueno tener alguna estructura que contuviera a \textit{las columnas} de $W$, para poder pedirlas en $O(1)$ y multiplicar fácilmente a cada una por un mismo escalar. \\

		En particular, vamos a querer multiplicar a sus elementos \textit{distintos de $0$} (pues los que son $0$ no haría falta porque quedarían iguales), y la matriz $W$, por ser rala, contiene muchos elementos iguales a $0$. Es por esto que pensamos usar un \textit{map$<$int, double$>$} para almacenar las columnas: donde el \textit{int} es el número de fila y el \textit{double} es el valor almacenado en esa posición del \textit{map} (es decir, de la columna). Nos evitaremos almacenar los $0$s, asumiendo que las posiciones que no están almacenadas son $0$. \\

		Por otro lado, para la triangulación del sistema lineal mediante el algoritmo de Eliminación Gaussiana, lo que sucede es más bien lo opuesto: cada paso del algoritmo es equivalente a \textit{pre}multiplicar (multiplicar por izquierda) al sistema $(I - pWD) x = e$ por un matriz elemental $E_{(i,j)}$ distinta. Estas matrices son como la matriz identidad, pero poseen un elemento distinto de $0$ en la posición $(i,j)$. \\

		\begin{equation}
			(I - pWD) x = e <=> E_{(n,n-1)} \hdots E_{(2,1)} (I - pWD) x = E_{(n,n-1)} \hdots E_{(2,1)} e
		\end{equation}

		Esto, a su vez, es equivalente a hacer operaciones con las filas. Por ejemplo, si $E_{(2,1)}$ tiene un escalar $m_{2,1} \neq 0$ en la posición $(2,1)$, premultiplicar $E_{(2,1)}$ por $(I - pWD)$ será modificar la segunda fila de $(I - pWD)$ de manera tal que: \\

		\begin{equation}
			fila_{2}(I-pWD) = m_{2,1} *  fila_{1}(I-pWD) + fila_{2}(I-pWD)
		\end{equation}

		Esto nos hizo pensar que, para hacer estas operaciones, sería más fácil tener a la matriz como \textit{map} \underline{de filas}. \\

		Entonces lo que resolvimos hacer fue cargar la matriz como \textit{map} de columnas para armar el sistema, y luego trasponerla e interpretarla como un \textit{map} de filas para la resolución del sistema lineal. \\

	\subsection{\textit{Page Rank} (ranking de Page/rango de página)}

		Como vimos en la introducción teórica, para implementar \textit{Page Rank} vamos a armar la matriz dispersa W, llamada \textit{de conectividad}, que contiene un 1 en la posición $(i,j)$ si la página \textit{j} enlaza a la página \textit{i}. \\

		El programa recibirá por parámetro la ruta del archivo que contiene la información de las páginas: en la primera línea la cantidad de enlaces, en la segunda la cantidad de páginas, y en la tercera y las subsiguientes, dos números separados por un espacio, que representan que existe un enlace desde el primero hacia el segundo. \\

		La cantidad de enlaces será usada para verificación, la cantidad de páginas la usaremos para definir el tamaño de la matriz de conectividad y, a medida que vayamos leyendo los enlaces, escribiremos $1$ en la posición de $W$ correspondiente, al tiempo que sumaremos $1$ al contador ubicado en la fila $i$ del vector $c$ de grados. \\

		Luego, para representar la matriz $-pD$, usaremos un \textit{vector$<$double$>$} en el que solo almacenaremos los elementos de su diagonal (pues el resto son $0$). \\

		Para multiplicar (por derecha) a $W$ por $-pD$, haremos un ciclo que recorrerá el vector en el que almacenamos $-pD$, multiplicando en cada paso a la columna de $W$ correspondiente. Para esto, usará el método público de nuestra implementación de matriz dispersa: $multColByScalar$ (multiplicar columna por escalar). \\

		Por último, le sumamos $I$ para obtener $A = W(-pD) + I$, trasponemos, creamos el vector de $1$s $e$ y le pasamos $A$ y $e$ a nuestro método resolvedor de sistemas, que interpreta a las matrices como \textit{map} de filas. Por dentro, el resolvedor hará eliminiación gaussiana y substitución hacia adelante y hacia atrás, métodos que serán explicados a continuación. \\

		\subsubsection{Eliminación Gaussiana \textit{(gaussian elimination)}}

			El algoritmo de eliminación gaussiana factoriza a una matriz A en dos matrices: L triangular inferior con $1$s en la diagonal y $U$ triangular superior, tal que $A = LU$. \\

			Forma parte de la rutina resolvedora de sistemas del programa, porque llevar a una matriz a esta forma facilita mucho su resolución. \\

			No todas las matrices tienen factorización $A=LU$. Por lo que el algoritmo de eliminación gaussiana tal como está planteado (sin pivoteo o naíf) no tendría, en principio, por qué funcionar. \\

			En este caso, su funcionamiento está garantizado porque la matriz $A$ es estrictamente diagonal dominante (en particular, por columnas). \\

			%\subsubsubsection{Implementación} % no such command :(
			\quad \\

				Con respecto a su implementación: en un principio, programamos y verificamos el funcionamiento de este método, basándonos en una clase matriz previa más simple, implementada con un \textit{array}. \\

				Como todavía no sabíamos que la matriz $(I-pWD)$, por sus propiedades numéricas, tendría factorización LU sin pivoteo; programamos eliminación gaussiana con pivoteo parcial. \\ % acá tiene que ir una referencia al apéndice donde está la demostración de esto

				Lo que hicimos luego, fue programar los mismos métodos públicos que en la primera implentación de matriz (como \textit{array}), en la nueva clase matriz dispersa (\textit{map$<$int, map$<$int, double$>$ $>$}), para poder utilizar el mismo algoritmo de eliminación gaussiana, sin hacerle demasiados cambios. \\

				Decidimos conservar el pivoteo parcial, ya que esto le otorga mayor estabilidad numérica a los cálculos. \\ % acá tiene que ir referencia a alguna fuente

		\subsubsection{Substitución Hacia Adelante y Hacia Atrás\\ \textit{(forwards and backwards substitution)}}

			Una ecuación matricial de la forma $Lx = b$ o $Ux = b$ (L triangular inferior, U triangular superior) es fácilmente soluble mediante un método iterativo llamado sustitución hacia adelante (o sustitución hacia atrás, respectivamente). \\

			En este proceso, por ejemplo para matrices triangulares inferiores, uno primero soluciona $x_1$ (el primer elemento del vector columna de incógnitas), luego usa $x_1$ para encontrar $x_2$ en la ecuación siguiente (por eso \textit{hacia adelante}) y así sucesivamente hasta $x_n$. \\

			La primera ecuación del sistema $Lx = b$: \\

			\begin{equation}
				l_{1,1} x_{1} = b_{1} => x_{1} = \frac{b_{1}}{l_{1,1}} \qquad \text{(primer paso de la substitución hacia adelante)}
			\end{equation}

			Para matrices triangulares superiores, el proceso es análogo, pero reemplazando primero $x_n$ y hasta $x_1$ (hacia atrás). \\

			Para resolver el sistema de Page Rank, luego de obtener la factorización LU de nuestra matriz A: \\

			\begin{equation}
				Ax = b <=> LUx = b
			\end{equation}

			Para evitar los problemas numéricos que podría traer calcular la inversa de la matriz L, vamos a aplicar la siguiente sustitución: \\

			\begin{equation}
				Ux = y
			\end{equation}

			Entonces el sistema a resolver quedará escrito de la siguiente manera: \\

			\begin{equation}
				Ly = b
			\end{equation}

			Donde podemos usar sustitución hacia adelante para obtener $y$, y finalmente resolver $x$ haciendo sustitución hacia atrás en el sistema: \\

			\begin{equation}
				Ux = y
			\end{equation}

			%\subsubsubsection{Implementación}
			\quad \\

				Con respecto a su implementación: este método también lo programamos y lo probamos en un principio usando una clase matriz basada en un \textit{array}, que luego fue reemplazada por una clase matriz dispersa con los mismos métodos públicos. \\

				Tanto cuando hacemos sustitución hacia adelante como hacia atrás, pasamos elementos de la diagonal de la matriz triangular dividiendo, con el fin de despejar los $x_i$. Por lo tanto, para garantizar que los algoritmos funcionen, necesitamos poder asegurar que los elementos de la diagonal de las matrices triangulares involucradas sean distintos a $0$. \\

				Esto es cierto para la matriz $L$ triangular inferior, porque la matriz $L$ de la factorización $LU$ siempre tiene $1$s en la diagonal. \\

				Para la matriz $U$ triangular superior, el argumento es un poco más complejo: sabemos que la matriz $A$ es inversible por ser estrictamente diagonal dominante. Luego: \\ % probar lo de edd=>inversible en un apéndice y poner una referencia acá

				\begin{equation}
					det(A) = det(LU) = det(L)det(U) = 1.det(U) = det(U)
				\end{equation}

				Además, como $U$ es triangular: \\

				\begin{equation}
					det(U) = \prod_{i=1}^{n} u_{ii}
				\end{equation}

				Y como $A$ es inversible: \\

				\begin{equation}
					det(A) \neq 0
				\end{equation}

				Por lo tanto: \\

				\begin{equation}
					det(A) = det(U) = \prod_{i=1}^{n} u_{ii} \neq 0 => u_{ii} \neq 0, \forall i
				\end{equation}

				Y queda garantizado el funcionamiento del algoritmo también para estas matrices. \\

		\subsubsection{Planteo de mediciones experimentales}

			Para entender mejor el funcionamiento de \textit{Page Rank}, vamos a proponer estudiar instancias de prueba con las siguientes familias de matrices de conectividad:\\

			1) \underline{Ciclo}: nos interesa la $W$ con una diagonal todo en unos porque hace que la matriz $D$ tenga todos los $d_{jj}$ iguales, haciendo que $p$ sea el unico factor de la ecuación de \textit{Page Rank} que determine en que página estará el navegante luego de $n$ iteraciones (además de la cantidad de iteraciones). Si $p$ (probabilidad de que salte a una página al azar) fuese $0$ (no puede ser $0$, pero si está muy cerca de $0$), entoces lo único que determinará dónde estará el navegante luego de iteraciones será $n$ y donde empieza el navegante. \\

			2) \underline{Ciclo y grafo completo}: nos interesa tener una $W$ que tiene un ciclo muy grande (como la descripta arriba) y un grafo completo mucho más chico. De esta forma, estamos representando dos islas de páginas que se señalan entre si: una en la que cáda página recibe y envía un único enlace y otra, mucho más chica pero en la que todos los elementos envían y reciben enlaces de todas las páginas del grafo. Con esto pretendemos que el peso de cada página del grafo sea mucho más grande que el de la lista. Con esto pretendemos experimentar hasta que punto importa el tamaño del ciclo (que se verá beneficiada por un $p$ más alto -porque al saltear aleatoriamente tiene muchas más chances de caer en una página de la ciclo (por tener la mayoría de las páginas de $W$ en el ciclo)) respecto del grafo completo. Es decir, para diferentes $p$, podremos ver hasta donde importa el tamaño del ciclo respecto del peso de los elementos del grafo. \\

			3) \underline{Lista no-dirigida}: idem ciclo pero el último elemento no apunta al primero. Esperamos que el \textit{ranking} quede (si el $p$ es suficientemente bajo): el último elemento, ... el primer elemento. Porque sin importar a que página salte o en que página esté, tiene mayor probabilidad de avanzar a la siguiente, entonces todas convergen a la última. Y cuando está en la última, sin importar el $p$, va a tener que saltar a una de las páginas de la lista con probabilidad $1/n-1$.\\

			4) \underline{Completa}: Todas las páginas deberían estar en la posición $1$ del \textit{ranking} Page. ($W$ tiene todas las posición igual a $1$ -excepto diagonal-).\\

			5) \underline{Columna completa y el resto vacía}: hipótesis: la página que manda enlaces a todas las páginas, debe quedar última en el pagerank.

			% Columna completa y un link apuntando a la pág que manda links:Tambien nos inetersa el caso anterior pero que la página que manda todos los liks tambien reciba un link. Hip la pág que envía los links es la única que recibe un link de una pág que recibe un link, entonces debería terminar con la pág que manda todos los links como numero uno e el ranking.

